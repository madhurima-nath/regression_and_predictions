---
title: "Regression and Predictions"
output:
  html_notebook: default
---

## Contents:

This notebook contains the following topics:

-   [Linear Regression]

    -   [Simple Linear Regression]
    -   [Multiple Linear Regression]

-   [Real-world Example using Regression]

-   [Factor Variables]

-   [Regression Diagnostics - Outliers, Influential Values, Correlated Errors]

-   [Polynomial and Spline Regression]

-   [Additional Points to Remember]

## Linear Regression

[**Aim**]{.ul}: to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, to estimate the value of the response (outcome of dependent) variable $y$, when only the predictors (explanatory or independent variables) $x$ values are known.

Given a data set of $n$ data points, $\{y_{i},\,x_{i1},\ldots ,x_{ip}\}_{i=1}^{n},$ the linear model with $p$-vector of predictors $x$ is:

${y}_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+ \epsilon_{i} = \mathbf{x}_{i}^{\mathsf {T}}{\boldsymbol {\beta }} + \epsilon_{i},\qquad i=1,\ldots ,n,$

where $\mathbf{x}_{i}^{\mathsf {T}} \beta$ is the inner product between vectors $\mathbf{x}_i$ and $\boldsymbol {\beta}$ and $\epsilon$ is the error variable, an unobserved random variable that adds 'noise' to the linear relationship.

In matrix notation, ${\mathbf{y = X}} {\boldsymbol{\beta}} + {\bf{\epsilon}}$, where,

$\mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\.\\.\\.\\ y_n \end{pmatrix}, \qquad {\boldsymbol {\beta}} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\.\\.\\.\\ \beta_p \end{pmatrix}, \qquad {\bf{\epsilon}} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\.\\.\\.\\ \epsilon_n \end{pmatrix}$

${\bf{X}} = \begin{pmatrix} \bf{x}_1^{\mathsf{T}} \\ \bf{x}_2^{\mathsf{T}} \\.\\.\\.\\ \bf{x}_n^{\mathsf{T}} \end{pmatrix} = \begin{pmatrix} 1 & x_{11} & \ldots & x_{1p} \\ 1 & x_{21} & \ldots & x_{2p} \\. & . & \ldots & .\\. & . & \ldots & .\\. & . & \ldots & . \\ 1 & x_{n1} & \ldots & x_{np} \end{pmatrix}$

Summarize:

-   $\mathbf{X}$ - independent predictors
-   $\mathbf{y}$ - dependent response
-   $\boldsymbol{\beta}$ - regression coefficients
-   linear relationship between the response function and regression coefficients
-   distribution of $\mathbf{X}$ is arbitrary
-   distribution of estimated coefficients, $\widehat{\boldsymbol{\beta}}$ will depend on the distribution of predicted responses, $\widehat{\mathbf{y}}$

The best fitted model - one which minimizes the sum of the prediction errors/residuals, i.e., minimizes $\sum_{i=0}^n e_i = \sum_{i=0}^n (y_i - \widehat{y}_i)^2$

(i.e., minimizes the least squared error). Here $\widehat{y}_i$ are the predicted responses.

#### [Linear Regression has the following assumptions:]{.ul}

-   mean of the response at each value of the predictor, $x_i$, is a *linear* function of the $x_i$.
-   errors/residuals, $\epsilon_i$, are *identically and independently distributed* as normal distribution, with mean 0 and equal (unknown) variance.
-   errors, $\epsilon_i$, at each value of $x_i$, are *normally distributed.*
-   responses, $y_i$ and therefore, $\beta_i$, are *normally distributed*.

### Simple Linear Regression

Simple linear regression attempts to model the data in the form of the best fitting line:

${y}_i = \beta_0 + \beta_1 x_i + \epsilon_i \qquad i = 1,‚Ä¶, n$

This is the equation of a straight line, recall $y = mx +c$, where $m$ is the slope and $c$ is the intercept.

Minimization $\rightarrow$ take derivatives of $\sum_{i=1}^n e_i$ w.r.t $\beta_0$ and $\beta_1$, set to 0 and solve for $\beta_0$ and $\beta_1$.

$\widehat\beta_0 = \overline{y} - \widehat\beta_1 \overline{x}$

$\widehat\beta_1 = \frac{\sum_{i=1}^n (x_i - \overline{x} )(y_i - \overline{y} )}{\sum_{i=1}^n (x_i - \overline{x})^2}$

where,

$\overline{x}$ is the mean of all of the x-values

$\overline{y}$ is the mean of all of the y-values

$\widehat\beta_0$ tells the estimated regression equation at $x = 0$, if the 'scope of the model' includes $x = 0$, otherwise, $\widehat\beta_0$ is not meaningful.

$\widehat\beta_1$ is the amount by which the mean response vary for every one unit increase in $x$.

### Multiple Linear Regression

Multiple linear regression is a generalized version of simple regression, where more than one independent variables are used. . The basic model for multiple linear regression is as discussed above:

${y}_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+ \epsilon_{i} = \mathbf{x}_{i}^{\mathsf {T}}{\boldsymbol {\beta }} + \epsilon_{i},\qquad i=1,\ldots ,n,$

The solution for the coefficients can be easily calculated using the matrix formulation: $\hat{\boldsymbol\beta} = (\mathbf{x}^T \mathbf{x})^{-1} \mathbf{x}^T \mathbf{y}$

Generally, all real-world problems involve multiple predictors.

## Real-world Example using Regression

Data: New York air quality data in R ([details about dataset](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airquality.html))

-   Ozone: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

-   Solar.R: Solar radiation in Langleys in the frequency band 4000--7700 Angstroms from 0800 to 1200 hours at Central Park

-   Wind: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

-   Temp: Maximum daily temperature in degrees Fahrenheit at LaGuardia Airport

-   Month: Numeric value of Month (1--12)

-   Day: Day of month (1--31).

There are 6 variables - our target $\rightarrow$ attribute 'ozone'.

$lm(formula,data)$ function creates the relationship model between the predictor and the response variable, where the parameters used are -

-   formula is a symbol presenting the relation between x and y.
-   data is the vector on which the formula will be applied.

$predict(object, newdata)$ function is used for prediction, where the parameters used ‚àí

-   object is the formula which is already created using the lm() function.
-   newdata is the vector containing the new value for predictor variable.

```{r}
data('airquality')
str(airquality)
```

```{r}
head(airquality)
```

#### Exploratory Analysis and Pre-processing the data

Check for missing values in the dataset.

```{r}
mapply(anyNA, airquality)
```

There are some missing values in columns 'Ozone' and 'Solar.R'. Let's impute these by replacing the *'NA'*¬†with monthly average.

```{r}
for (i in 1:nrow(airquality)){
  if(is.na(airquality[i,'Ozone'])){
    airquality[i,'Ozone'] = mean(airquality[
      which(airquality[, 'Month'] == airquality[i, 'Month']), 
            'Ozone'],na.rm = TRUE)
  }
  
  if(is.na(airquality[i,'Solar.R'])){
    airquality[i,'Solar.R'] = mean(airquality[
      which(airquality[, 'Month'] == airquality[i, 'Month']), 
            'Solar.R'],na.rm = TRUE)
  }
  
}
```

```{r}
head(airquality)
```

#### Simple Regression on the data

Since simple regression requires only one predictor, let's take 'Temp' as the predictor for the target 'Ozone'.

```{r}
# select predictor attribute
x =  airquality[, 'Temp']
                
# select target attribute
y = airquality[, 'Ozone'] 
```

```{r}

simple_model  = lm(y~x)

# provides regression line coefficients, i.e., slope and y-intercept
simple_model
```

```{r}

# scatter plot between x and y
plot(y~x)
# plot the regression line
abline(simple_model, col = 'blue', lwd = 2)
```

```{r}
summary(simple_model)
```

#### Interpretation of the output

**Call**: Formula used to fit the data.

**Residuals**: Difference between the actual observed response values and the response values that the model predicted. This section broken into 5 summary points. A symmetrical distribution across these points on the mean value zero (0) tells how well the model fits the data. In the example, we can see that the distribution of the residuals do not appear to be strongly symmetrical. That means that the model predicts certain points that fall far away from the actual observed points. Plotting the residuals will help check whether they are normally distributed.

**Coefficients**: This section tells the coefficients of the model. In simple linear regression, the coefficients are two unknown constants that represent the *intercept* and *slope* terms in the linear model.

The coefficient standard error measures the average amount that the coefficient estimates vary from the actual average value of our response variable.

The coefficient t-value is a measure of how many standard deviations the coefficient estimate is far away from 0. The farther away from zero indicates that the null hypothesis (there is no relationship between the predictor and the response) can be rejected - that is, declare a linear relationship between ozone numbers and temperature exists.

The Pr(\>\|t\|) relates to the probability of observing any value equal or larger than $t$. A small p-value indicates that it is unlikely to observe a relationship between the predictor (temperature) and response (ozone) variables due to chance. Typically, a p-value of 5% or less is a good cut-off point. In this example, the p-values are very close to zero. \
Note: 'Signif. Codes' associated to each estimate. Three stars (or asterisks) represent a highly significant p-value. Consequently, a small p-value for the intercept and the slope indicates that the null hypothesis can be rejected.

**Residual Standard Error**: measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term. The residual standard error is the average amount that the response will deviate from the true regression line.

**Multiple R-squared, Adjusted R-squared**: R-squared statistic provides a measure of how well the model is fitting the actual data. It takes the form of a proportion of variance, lies between 0 and 1.

In multiple regression settings, the $R^2$ will always increase as more variables are included in the model. That's why the adjusted $R^2$ is the preferred measure as it adjusts for the number of variables considered.

**F statistic**: tells if there is a relationship between the dependent and independent variables being tested. Generally, a large F indicates a stronger relationship. The further the F-statistic is from 1 the better it is. However, how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors. Generally, when the number of data points is large, an F-statistic that is only a little bit larger than 1 is already sufficient to reject the null hypothesis

```{r}

library(ggplot2)

p = ggplot(data = airquality, aes(x, y)) + geom_point() +
stat_smooth(method = 'lm', col = 'dodgerblue3') +
theme(panel.background = element_rect(fill = 'white'),
axis.line.x=element_line(),
axis.line.y=element_line()) + ggtitle('Linear Model Fitted to Data')

p + labs(x = 'Temp', y = 'Ozone')
```

The gray shading around the line represents a confidence interval of 0.95, the default for the stat_smooth() function, which smoothes data to make patterns easier to visualize. This 0.95 confidence interval is the probability that the true linear model for the temperature and ozone will lie within the confidence interval of the regression model fitted to the data. There is still variability within the observations, even though the model fits the data well.

```{r}
ggplot(data = airquality, aes(simple_model$residuals)) +
geom_histogram(bins = 50, color = 'black', fill = 'grey') +
theme(panel.background = element_rect(fill = 'white'),
axis.line.x=element_line(),
axis.line.y=element_line()) +
ggtitle('Histogram for Model Residuals')

```

#### Multiple Regression on the data

```{r}

# select predictor attribute
x =  data.matrix(airquality[, 2:6])
                
# select target attribute
y = airquality[, 'Ozone']
```

```{r}
multi_model = lm(y~x, data = airquality)

summary(multi_model)
```

## Factor Variables

These are categorical variables - can be either numeric or string variables.

Most important advantage of converting categorical life to factor variable: they can be used in statistical modeling where they will be implemented correctly, i.e., storing data as factors ensures that the modeling functions will treat such data correctly, and then be assigned the correct number of degrees of freedom.

In the above example,

## Regression Diagnostics - Outliers, Influential Values, Correlated Errors

**Outliers**: an observation which has a response value that is very different from the predicted value based on a model.

**Influential values**: a data point which unduly influences any part of a regression analysis, such as the predicted responses, the estimated slope coefficients, or the hypothesis test results.

-   linear regression model is sensitive to outliers and high-leverage points (data points which have extreme predictor values)

-   methods to identify influential values:

    1.  Difference in Fits (DFFITS)

    2.  Cook's Distances

-   In both methods, the following idea is used:

    Delete the observations one at a time, refit the regression model on the remaining ùëõ‚Äì1n--1 observations each time. Then, compare the results using all ùëõn observations to the results with the ùëñi-th observation deleted to see how much influence the observation has on the analysis. This enables to assess the potential impact each data point has on the regression analysis.

**Correlated errors**: errors

## Polynomial and Spline Regression

text

## Additional Points to Remember

Multi-collinearity:

Over-fitting and under-fitting are two extreme ends of fitting a model to the data observations.

-   Under-fitting: when model doesn't capture the underlying trend from the data points. e.g.: trying to model a non-linear relationship with a linear model. This scenario is the case of high bias and low variance.
-   Over-fitting: when model tries to fit all the data points, that it starts capturing the noise. e.g.: trying to model a quadratic relationship with a 5th order polynomial. This is the case of low bias and high variance.

\- Bias: it is the error from incorrect assumptions in the learning algorithm.

\- Variance: it is the error from sensitivity to small fluctuations in the training set.

Frequently used techniques to reduce under-fitting:

-   Increase model complexity
-   Increase number of features, perform feature engineering
-   Remove noise from the data
-   Increase the number of epochs or increase the duration of training to get better results.

Frequently used techniques to reduce over-fitting:

-   Increase training data
-   Reduce model complexity
-   Early stopping during the training phase (keep an eye over the loss over the training period as soon as loss begins to increase stop training).
-   Ridge Regularization and Lasso Regularization - penalize for additional complexity

```{r}

# split into train-test set
#library(caTools)
#x_train = subset(x, sample.split(airquality, SplitRatio = 0.7) == TRUE)
#y_train = subset(y, sample.split(airquality, SplitRatio = 0.7) == TRUE)

#x_test = subset(x, sample.split(airquality, SplitRatio = 0.7) == FALSE)
#y_test = subset(y, sample.split(airquality, SplitRatio = 0.7) == FALSE)
```

## References

1.  [Penn State STAT 462 Applied Regression Analysis](https://online.stat.psu.edu/stat462/node/77/)
2.  [Khan Academy](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/influential-points-regression))
3.  [Cornell Machine Learning Course](https://www.cs.cornell.edu/courses/cs4780/2018fa/)
4.  [CMU Data Analytics Course](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf)
